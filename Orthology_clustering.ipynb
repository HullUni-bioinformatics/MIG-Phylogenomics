{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Make orthology clusters\n",
    "## 3.1 Run OrthoFinder\n",
    "### 3.1.1 Set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "misc.py:291: UserWarning: keeping existing fpath orthofinder/all_inputs\n",
      "  warnings.warn('keeping existing fpath %s'%name)\n"
     ]
    }
   ],
   "source": [
    "import misc, os\n",
    "\n",
    "software_fpath = 'python OrthoFinderExe'\n",
    "\n",
    "# I use a parallelized version by @sujaikumar not yet pulled\n",
    "# to the master branch in OrthoFinder\n",
    "# https://github.com/davidemms/OrthoFinder/pull/17\n",
    "\n",
    "misc.makedir('orthofinder')\n",
    "\n",
    "# link files into the input direcotry\n",
    "orth_base = './orthofinder'\n",
    "\n",
    "lncline = \"ln -s %s %s\"\n",
    "\n",
    "import misc, glob\n",
    "# make an input subdirectory for the minimal dataset\n",
    "misc.makedir('orthofinder/all_inputs')\n",
    "\n",
    "\n",
    "\n",
    "# reference samples\n",
    "my_samples = ['MareHarA','MincW1','MjavVW4', 'MfloSJF1', 'MentL30', 'MfloJB5']\n",
    "prots_dir = \"all_protein_ref_reviewed\"\n",
    "for s in my_samples:\n",
    "    real_path = \"%s/%s_ref.aa.fasta\"%(prots_dir, s)\n",
    "    ln_path   = '%s/all_inputs/%s_ref.aa.fasta'%(orth_base, s)\n",
    "    \n",
    "    if not os.path.exists(ln_path):\n",
    "        misc.printoe(\n",
    "            misc.execute_cline(lncline%(real_path, ln_path))\n",
    "        )\n",
    "        \n",
    "# abad sample\n",
    "\n",
    "#abad_fpath = \"./GS978784/mi.protein.faa\"\n",
    "#ln_path   = \"%s/all_inputs/mi.protein.faa\"%orth_base\n",
    "\n",
    "if not os.path.exists(ln_path):\n",
    "    misc.printoe(\n",
    "        misc.execute_cline(lncline%(abad_fpath, ln_path))\n",
    "    )\n",
    "# mapped samples\n",
    "prots_dir = \"./all_proteins\"\n",
    "for f in glob.glob(prots_dir+'/*.aa.fasta'):\n",
    "    s = f.split('/')[-1].split('.')[0]\n",
    "    real_path = \"%s/%s.aa.fasta\"%(prots_dir, s)\n",
    "    ln_path   = '%s/all_inputs/%s.aa.fasta'%(orth_base, s)\n",
    "    \n",
    "    if not os.path.exists(ln_path):\n",
    "        misc.printoe(\n",
    "            misc.execute_cline(lncline%(real_path, ln_path))\n",
    "        )\n",
    "\n",
    "## Run orthofinder\n",
    "# cline for minimal analysis\n",
    "in_fpath = orth_base + '/all_inputs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Iterate over inflation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2902: DtypeWarning: Columns (1,2,3,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inflation</th>\n",
       "      <th>one to ones</th>\n",
       "      <th>two to twos</th>\n",
       "      <th>between one and four in all samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.5</td>\n",
       "      <td>157</td>\n",
       "      <td>266</td>\n",
       "      <td>4448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.1</td>\n",
       "      <td>43</td>\n",
       "      <td>210</td>\n",
       "      <td>2824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>220</td>\n",
       "      <td>236</td>\n",
       "      <td>4243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>283</td>\n",
       "      <td>214</td>\n",
       "      <td>3851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>291</td>\n",
       "      <td>196</td>\n",
       "      <td>3582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>298</td>\n",
       "      <td>176</td>\n",
       "      <td>3358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>304</td>\n",
       "      <td>162</td>\n",
       "      <td>3179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>307</td>\n",
       "      <td>160</td>\n",
       "      <td>2996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>309</td>\n",
       "      <td>154</td>\n",
       "      <td>2849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20.0</td>\n",
       "      <td>273</td>\n",
       "      <td>106</td>\n",
       "      <td>2213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   inflation  one to ones  two to twos  between one and four in all samples\n",
       "0        1.5          157          266                                 4448\n",
       "1        1.1           43          210                                 2824\n",
       "2        2.0          220          236                                 4243\n",
       "3        3.0          283          214                                 3851\n",
       "4        4.0          291          196                                 3582\n",
       "5        5.0          298          176                                 3358\n",
       "6        6.0          304          162                                 3179\n",
       "7        7.0          307          160                                 2996\n",
       "8        8.0          309          154                                 2849\n",
       "9       20.0          273          106                                 2213"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, misc\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# first run, will do the blast step\n",
    "orthofinder_exe = '%s/%i_orthofinder_parallel_beta_2bb1fe3.py'%(software_fpath, 1)\n",
    "iter_cline = orthofinder_exe+\" -f %s  -t 15\"%in_fpath\n",
    "misc.printoe(\n",
    "    misc.execute_cline(iter_cline)\n",
    "    )\n",
    "\n",
    "\n",
    "inflation_to_use = 2 # this is an apreiori decision, based on table below \n",
    "\n",
    "# Subsequent runs, reuse blast results\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(0,9) + [20]:\n",
    "    # There are 9 copies of the orthofinder python script, numbered 0-8,20.\n",
    "    # e.g., 2_orthofinder_parallel_beta_2bb1fe3.py.\n",
    "    # The  numbers are also the mcl inflation value as set in the script\n",
    "    # except for script 0, where the inflation value is the default (1.5)\n",
    "    # and 1, where the inflation value is 1.1\n",
    "    inflation_value = i\n",
    "    if i == 1:\n",
    "        inflation_value = 1.1\n",
    "    elif i == 0:\n",
    "        inflation_value = 1.5\n",
    "    \n",
    "    # iter_results collects statistics from this iteration,\n",
    "    # which are shown in the output table\n",
    "    iter_results = [inflation_value]\n",
    "    \n",
    "    # this is the executable for this specific iteration\n",
    "    orthofinder_exe = '%s/%i_orthofinder_parallel_beta_2bb1fe3.py'%( \n",
    "                       software_fpath, i)\n",
    "    \n",
    "    # this is the command line for this specific iteration\n",
    "    iter_cline = orthofinder_exe+\" -b %s/%s/WorkingDirectory -t 15\"%(\n",
    "                                     in_fpath, 'Results_Jul02')\n",
    "    \n",
    "    # execute orthofinder\n",
    "    #out, err = misc.execute_cline(iter_cline)\n",
    "    \n",
    "    # rename the ortofinder csv output file to indicate the executable that was used\n",
    "    #os.rename('%s/%s/WorkingDirectory/OrthologousGroups.csv'%(in_fpath, 'Results_Jul02'),\n",
    "    #          '%s/%s/%i_OrthologousGroups.csv'%(in_fpath, 'Results_Jul02',i))\n",
    "    \n",
    "    # analyse the csv file to prepare the table below\n",
    "    \n",
    "    # path to csv\n",
    "    og_fpath = 'orthofinder/all_inputs/Results_Jul02/%i_OrthologousGroups.csv'%i\n",
    "    \n",
    "    #  read the csv to pandas dataframe\n",
    "    OGs = pd.read_table(og_fpath, index_col=0)\n",
    "    \n",
    "    # these are OG categories I want to count in the CSV file\n",
    "    one_to_ones = 0\n",
    "    two_to_twos = 0\n",
    "    between_one_and_four_in_all_samples = 0\n",
    "    \n",
    "    # The columns in the csv file (and in the dataframe)\n",
    "    # are named with the protein input files of all the samples\n",
    "    file_template = '%s_ref.aa.fasta'\n",
    "    \n",
    "    # I collect OGs I want to include in the phylogenetic analysis\n",
    "    # from the orthofinder run with inflation value 1.5 because\n",
    "    # I thought it was the best (but I ended up using inflation 4,\n",
    "    # when I analysed all tha samples, below)\n",
    "    if inflation_value == inflation_to_use:\n",
    "        OGs_to_get = []\n",
    "    \n",
    "    # count the OG categories and choose the ones to analyse\n",
    "    for ind,row in OGs.iterrows():\n",
    "        counts = {\n",
    "            'MareHarA':0,\n",
    "            'MincW1':0,\n",
    "            'MjavVW4':0,\n",
    "            'MfloSJF1':0,\n",
    "            'MentL30':0\n",
    "        }\n",
    "        \n",
    "        # in each column (ie, sample), the sequence IDs are seperated\n",
    "        # with comas\n",
    "        for sp in counts:\n",
    "            try:\n",
    "                counts[sp] = len(row[file_template%sp].split(','))\n",
    "            except:\n",
    "                counts[sp] = 0\n",
    "                \n",
    "        get = False\n",
    "        \n",
    "        if all([counts[sp] == 1 for sp in counts]):\n",
    "            one_to_ones += 1\n",
    "            get = True\n",
    "        elif all([counts[sp] == 2 for sp in counts]):\n",
    "            two_to_twos += 2\n",
    "            get = True\n",
    "        elif all([1 <= counts[sp] <= 4 for sp in counts]):\n",
    "            between_one_and_four_in_all_samples += 1\n",
    "            get = True\n",
    "\n",
    "        if inflation_value == inflation_to_use and get:\n",
    "            OGs_to_get.append(ind)\n",
    "    \n",
    "    iter_results.append(one_to_ones)\n",
    "    iter_results.append(two_to_twos)\n",
    "    iter_results.append(between_one_and_four_in_all_samples)\n",
    "    results.append(iter_results)\n",
    "\n",
    "# make the table\n",
    "header = ['inflation',\n",
    "          'one_to_ones'.replace('_',' '),\n",
    "          'two_to_twos'.replace('_',' '),\n",
    "          'between_one_and_four_in_all_samples'.replace('_',' ')]\n",
    "   \n",
    "df = pd.DataFrame(results, columns=header)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Merge inparalogues with phylogenetic analysis\n",
    "### 3.2.1 Make a genbank file with inflation 2 groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import misc, glob\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqFeature import SeqFeature, FeatureLocation\n",
    "from Bio.Alphabet import IUPAC\n",
    "\n",
    "og_fpath = 'orthofinder/all_inputs/Results_Jul02/2_OrthologousGroups.csv'\n",
    "gb_out = 'orthofinder/all_inputs/Results_Jul02/OGs_I2_1-4.gb'\n",
    "refprotpath = './all_protein_ref_reviewed/%s_ref.aa.fasta'\n",
    "refcdspath = './all_cds_ref_reviewed/%s_ref.cds.fasta'\n",
    "\n",
    "sequences = {'MfloSJF1': {'aa': SeqIO.to_dict(SeqIO.parse(refprotpath%'MfloSJF1','fasta')),\n",
    "                      'cds': SeqIO.to_dict(SeqIO.parse(refcdspath%'MfloSJF1','fasta'))},\n",
    "             'MjavVW4': {'aa': SeqIO.to_dict(SeqIO.parse(refprotpath%'MjavVW4','fasta')),\n",
    "                      'cds': SeqIO.to_dict(SeqIO.parse(refcdspath%'MjavVW4','fasta'))},\n",
    "             'MincW1': {'aa': SeqIO.to_dict(SeqIO.parse(refprotpath%'MincW1','fasta')),\n",
    "                      'cds': SeqIO.to_dict(SeqIO.parse(refcdspath%'MincW1','fasta'))},\n",
    "             'MareHarA': {'aa': SeqIO.to_dict(SeqIO.parse(refprotpath%'MareHarA','fasta')),\n",
    "                      'cds': SeqIO.to_dict(SeqIO.parse(refcdspath%'MareHarA','fasta'))},\n",
    "             'MentL30': {'aa': SeqIO.to_dict(SeqIO.parse(refprotpath%'MentL30','fasta')),\n",
    "                      'cds': SeqIO.to_dict(SeqIO.parse(refcdspath%'MentL30','fasta'))},\n",
    "             'MfloJB5': {'aa': SeqIO.to_dict(SeqIO.parse(refprotpath%'MfloJB5','fasta')),\n",
    "                      'cds': SeqIO.to_dict(SeqIO.parse(refcdspath%'MfloJB5','fasta'))},\n",
    "            }\n",
    "\n",
    "for f in glob.glob('all_cdss/*.cds.fasta'):\n",
    "    smpl = f.split('/')[1].split('.')[0]\n",
    "    protpath = './all_proteins/%s.aa.fasta'%smpl\n",
    "    sequences[smpl] = {\n",
    "        'aa': SeqIO.to_dict(SeqIO.parse(protpath,'fasta')),\n",
    "        'cds': SeqIO.to_dict(SeqIO.parse(f,'fasta'))\n",
    "    }\n",
    "\n",
    "gb = open(gb_out,'wt')\n",
    "\n",
    "OGs = pd.read_table(og_fpath, index_col=0)\n",
    "\n",
    "filenames = [i.split('/')[-1] for i in glob.glob('./all_protein_ref_reviewed/*')]\n",
    "filenames += [i.split('/')[-1] for i in glob.glob('./all_proteins/*')]\n",
    "\n",
    "\n",
    "# write to genbank file\n",
    "for ind,row in OGs.iterrows():\n",
    "    if not ind in OGs_to_get:\n",
    "        continue\n",
    "            \n",
    "    for f in filenames:    \n",
    "        organism = f.split('_')[0].split('.')[0]\n",
    "        try:\n",
    "            ids = [x.strip().rstrip() for x in row[f].split(',')]\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "        for i in ids:\n",
    "            cdsr = sequences[organism]['cds'][i]\n",
    "                    \n",
    "            num_ambig = len([p for p in str(cdsr.seq) if not p in 'atgcATGC'])\n",
    "            if num_ambig > 9:\n",
    "                continue\n",
    "                \n",
    "            if len(cdsr.seq) < 60:\n",
    "                continue\n",
    "                \n",
    "            cdsr.seq.alphabet = IUPAC.ambiguous_dna\n",
    "            protr = sequences[organism]['aa'][i]\n",
    "            if str(protr.seq).count('x')+str(protr.seq).count('X')+str(protr.seq).count('*') > 3:\n",
    "                continue\n",
    "            source = SeqFeature(location=FeatureLocation(0, len(cdsr.seq)),\n",
    "                                type='source',\n",
    "                                qualifiers={'organism': [organism],\n",
    "                                            'original_id': [cdsr.id]})\n",
    "            \n",
    "            cdsr.features.append(source)\n",
    "            cds = SeqFeature(location=FeatureLocation(0, len(cdsr.seq)),\n",
    "                             type='CDS',\n",
    "                             qualifiers={'gene': [ind],\n",
    "                                         'translation': [str(protr.seq).replace('*','X')]})\n",
    "            cdsr.features.append(cds)\n",
    "            cdsr.id = organism +'_'+ cdsr.id\n",
    "            gb.write(cdsr.format('gb'))\n",
    "    \n",
    "gb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Load the records into a reprophylo project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reprophylodev import *\n",
    "\n",
    "# If you did not generate OGs_I2_1-4.gb yourself and want to use the provided one\n",
    "# you have to decompress it:\n",
    "# !gzip -d orthofinder/all_inputs/Results_Jul02/OGs_I2_1-4.gb.gz\n",
    "\n",
    "list_loci_in_genbank(gb_out,gb_out+'.loci.csv',gb_out+'.loci.txt')\n",
    "pj = Project(gb_out+'.loci.csv', pickle=gb_out+'.pkpj', git=False)\n",
    "\n",
    "gb_out\n",
    "\n",
    "pj.read_embl_genbank([gb_out])\n",
    "\n",
    "pj.extract_by_locus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Distribution of copy numbers in each orthology cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f56d0f83a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "copy_counts = [len(pj.records_by_locus[i]) for i in pj.records_by_locus]\n",
    "\n",
    "a=plt.hist(copy_counts, bins=100)\n",
    "min(copy_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Prepare fasta input files\n",
    "This will only write input fasta files for each orthology cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AlnConf(pj,\n",
    "        loci = [l.name for l in pj.loci],\n",
    "        method_name='linsiNoCDSAlign',\n",
    "        program_name='mafft', \n",
    "        CDSAlign=False,  \n",
    "        cline_args={'localpair':True, 'maxiterate': 1000})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5 Save\\ load the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle_pj(pj, gb_out+'.pkpj')\n",
    "# The pickled file is too large for github, but resulting fasta files are in there\n",
    "# in the directory I2_rootknot_phylogenomics/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "gb_out = 'orthofinder/all_inputs/Results_Jul02/OGs_I2_1-4.gb'\n",
    "\n",
    "with open(gb_out + '.pkpj', 'rb') as hndl:\n",
    "    pj = pickle.load(hndl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.6 Run phylogenetic workflow on viper\n",
    "The inputs and outputs of this are in the direcroty `I2_rootknot_phylogenomics` which was copied to and from the HPC.\n",
    "#### 3.2.6.1 Python script for the HPC\n",
    "  \n",
    "Since viper only runs up to 1000 array jobs at a time, this needs to be run four times with the following values in the variable `sorted_OGs`, at the top of the python script for the HPC above:\n",
    "\n",
    "`sorted_OGs = sorted(OGs)[0:]`  \n",
    "`sorted_OGs = sorted(OGs)[1000:]`  \n",
    "`sorted_OGs = sorted(OGs)[2000:]`  \n",
    "`sorted_OGs = sorted(OGs)[3000:]`  \n",
    "  \n",
    "**script starts here:**  \n",
    "\n",
    "<pre>\n",
    "from subprocess import Popen, PIPE\n",
    "from warnings import warn\n",
    "from sys import argv\n",
    "import glob\n",
    "from Bio import SeqIO\n",
    "\n",
    "job = int(argv[1])-1\n",
    "rp_id = argv[2]\n",
    "\n",
    "inputs = list(glob.glob(\"/home/457031/I2_rootknot_phylogenomics/raw/*.fasta\"))\n",
    "print(len(inputs))\n",
    "OGs = [i.rpartition('/')[-1].replace(rp_id+'_','').replace('.fasta','') for i in inputs]\n",
    "sorted_OGs = sorted(OGs)[4000:]\n",
    "OG = sorted_OGs[job]\n",
    "\n",
    "# Run mafft\n",
    "input_f = \"/home/457031/I2_rootknot_phylogenomics/raw/{0}_{1}.fasta\".format(rp_id, OG)\n",
    "output_f = \"/home/457031/I2_rootknot_phylogenomics/non_cds_alignment/{0}_{1}.aln.fasta\".\n",
    "format(rp_id, OG)\n",
    "\n",
    "mafft = \"mafft-7.299-with-extensions/core/mafft\"\n",
    "mafftcline = mafft + \" --thread 28 --localpair --maxiterate 1000 {0} > {1}\".format(input_f, output_f)\n",
    "\n",
    "input_f = \"/home/457031/I2_rootknot_phylogenomics/non_cds_alignment/{0}_{1}.aln.fasta\".f\n",
    "ormat(rp_id, OG)\n",
    "output_f = \"/home/457031/I2_rootknot_phylogenomics/non_cds_alignment-trimal0.7/{0}_{1}.a\n",
    "ln.trm.fasta\".format(rp_id, OG)\n",
    "\n",
    "p = Popen(mafftcline, shell=True, stdout=PIPE, stderr=PIPE)\n",
    "out, err = p.communicate()\n",
    "\n",
    "print(out)\n",
    "print(err)\n",
    "\n",
    "# Run trimal\n",
    "\n",
    "trimal = \"/home/457031/trimAl/source/trimal\"\n",
    "trimalcline = trimal + \" -in {0} -out {1} -gt 0.7 -st 0.001\".format(input_f,output_f)\n",
    "\n",
    "p = Popen(trimalcline, shell=True, stdout=PIPE, stderr=PIPE)\n",
    "out, err = p.communicate()\n",
    "\n",
    "print(out)\n",
    "print(err)\n",
    "\n",
    "# Check filter out alignments in which a single gene copy is split between contigs\n",
    "\n",
    "def is_bad_alignment(aln):\n",
    "    badaln = False\n",
    "    alnlength = aln.get_alignment_length()\n",
    "    for i in range(len(aln)):\n",
    "        if badaln:\n",
    "            break\n",
    "        iseqrecord = aln[i]\n",
    "        isp = iseqrecord.id.split('_')[0]\n",
    "        for j in range(i+1, len(aln)):\n",
    "            jseqrecord = aln[j]\n",
    "            jsp = jseqrecord.id.split('_')[0]\n",
    "            if isp == jsp:\n",
    "                overlap = 0\n",
    "                for x in range(alnlength):\n",
    "                    if aln[i,x] != '-' and aln[j,x] != '-':\n",
    "                        overlap += 1\n",
    "                if overlap < 20:\n",
    "                    badaln = True\n",
    "                    break\n",
    "    return badaln\n",
    "    \n",
    "    \n",
    "def check_alignment_file(f):\n",
    "    \n",
    "    from Bio import AlignIO\n",
    "    aln = AlignIO.read(f,'fasta')\n",
    "    aln_name = f.split('/')[-1].split('_')[0]\n",
    "    if is_bad_alignment(aln):\n",
    "        return aln_name\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "input_f = \"/home/457031/I2_rootknot_phylogenomics/non_cds_alignment-trimal0.7/{0}_{1}.aln.trm.fasta\".format(rp_id,OG)\n",
    "output_f = input_f.replace(\"non_cds_alignment-trimal0.7\", \"non_cds_alignment-trimal0.7-badaln\")\n",
    "\n",
    "baddies_path = \"/home/457031/I2_rootknot_phylogenomics/non_cds_alignment-trimal0.7-badaln/*_*.aln.trm.fasta\"\n",
    "bad_alns = [pt.rpartition('/')[-1].split('_')[1].split('.')[0] for pt in list(glob.glob(baddies_path))]\n",
    "\n",
    "# Run RAxML\n",
    "\n",
    "raxmlpthreads = \"/home/457031/standard-RAxML-master/raxmlHPC-PTHREADS-SSE3\"\n",
    "\n",
    "input_f = \"/home/457031/I2_rootknot_phylogenomics/non_cds_alignment-trimal0.7/{0}_{1}.aln.trm.fasta\".format(rp_id,OG)\n",
    "raxmlcline = raxmlpthreads + \" -w /home/457031/I2_rootknot_phylogenomics/non_cds_alignment-trimal0.7-tree  -p 123 -s {0} -T 28 -n {1}_{2} -m GTRGAMMA\".format(input_f, rp_id, OG)\n",
    "\n",
    "if not OG in bad_alns:\n",
    "    try:\n",
    "        p = Popen(raxmlcline, shell=True, stdout=PIPE, stderr=PIPE)\n",
    "        out, err = p.communicate()\n",
    "    except:\n",
    "        warn(OG + \" tree failed\")\n",
    "\n",
    "\n",
    "</pre>\n",
    "\n",
    "#### 3.2.6.1 Slurm script\n",
    "<pre>\n",
    "#!/bin/bash\n",
    "#SBATCH -J phylo_I2\n",
    "#SBATCH --array=1-1000\n",
    "#SBATCH -N 1\n",
    "#SBATCH --ntasks-per-node 1\n",
    "#SBATCH -o /home/457031/phylo.out\n",
    "#SBATCH -e /home/457031/phylo.err\n",
    "#SBATCH -p compute\n",
    "#SBATCH --exclusive\n",
    "\n",
    "#module add ncbi-blast/2.4.0\n",
    "module add gcc/4.9.3\n",
    "module add python/anaconda/4.0/2.7\n",
    "module add anaconda/4.0\n",
    "\n",
    "echo $SLURM_ARRAY_TASK_ID\n",
    "echo \"--\"\n",
    "python /home/my_viper_user/phylo.py $SLURM_ARRAY_TASK_ID 284421467500872.59\n",
    "\n",
    "module purge\n",
    "</pre>\n",
    "**This HPC workflow was repeated in the directory `I2_2X2_trimal0.8_rootknot_phylogenomics` with stricter trimal setting to analyse conversion tracts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.6 Collapse inparalogues and write fasta files based on the relaxed trimal settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "misc.py:294: UserWarning: fpath /home/amir/Dropbox/Nematodes/Meloidogyne_hybrids/Homeologue_phylogenomics/orthofinder/all_inputs/Results_Jul02/I2_3X2_gt0.7_st0.001_alns_1_4 newly created\n",
      "  warnings.warn('fpath %s newly created'%name)\n",
      "misc.py:294: UserWarning: fpath /home/amir/Dropbox/Nematodes/Meloidogyne_hybrids/Homeologue_phylogenomics/orthofinder/all_inputs/Results_Jul02/I2_3X2_gt0.7_st0.001_alns_flo2 newly created\n",
      "  warnings.warn('fpath %s newly created'%name)\n",
      "misc.py:291: UserWarning: keeping existing fpath /home/amir/Dropbox/Nematodes/Meloidogyne_hybrids/Homeologue_phylogenomics/orthofinder/all_inputs/Results_Jul02/I2_3X2_gt0.7_st0.001_alns_all2\n",
      "  warnings.warn('keeping existing fpath %s'%name)\n",
      "DEBUG:Cloud:Log file (/home/amir/.picloud/cloud.log) opened\n",
      "DEBUG:Cloud:Deleting 21485.log (7.20297363165 days old)\n",
      "DEBUG:Cloud:Deleting 9523.log (7.8606143169 days old)\n",
      "DEBUG:Cloud:Deleting 21485.lock (7.20297363165 days old)\n",
      "DEBUG:Cloud:Deleting 9523.lock (7.8606143169 days old)\n"
     ]
    }
   ],
   "source": [
    "from reprophylodev import *\n",
    "from collections import Counter\n",
    "import pickle, misc, os, pickle\n",
    "from fastcluster import ward\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "def hclust(linkmat, nclusters):\n",
    "    \"\"\"\n",
    "    based on the treeCl _hclust function\n",
    "    \"\"\"\n",
    "    linkmat_size = len(linkmat)\n",
    "    if nclusters <= 1:\n",
    "        br_top = linkmat[linkmat_size - nclusters][2]\n",
    "    else:\n",
    "        br_top = linkmat[linkmat_size - nclusters + 1][2]\n",
    "    if nclusters >= len(linkmat):\n",
    "        br_bottom = 0\n",
    "    else:\n",
    "        br_bottom = linkmat[linkmat_size - nclusters][2]\n",
    "    threshold = 0.5 * (br_top + br_bottom)\n",
    "    assignments = fcluster(linkmat, threshold, criterion='distance')\n",
    "    return assignments\n",
    "\n",
    "base += './orthofinder/all_inputs/Results_Jul02/'\n",
    "\n",
    "# Orthology clusters with 1 to 4 copies in all the reference samples\n",
    "alns_1_4 = base+'I2_3X2_gt0.7_st0.001_alns_1_4'\n",
    "\n",
    "# Orthology clusters with 2 copies in MfloSJF1 and 1 to 4 copies in all the other reference samples\n",
    "alns_flo2 = base+'I2_3X2_gt0.7_st0.001_alns_flo2'\n",
    "\n",
    "# Orthology clusters with 2 copies in all the reference samples\n",
    "alns_all2 = base+'I2_3X2_gt0.7_st0.001_alns_all2'\n",
    "gb_out = 'orthofinder/all_inputs/Results_Jul02/OGs_I2_1-4.gb'\n",
    "\n",
    "for alndir in [alns_1_4,alns_flo2,alns_all2]:\n",
    "    misc.makedir(alndir)\n",
    "    \n",
    "with open(gb_out+'.pkpj','rb') as hndl:\n",
    "    pj = pickle.load(hndl)\n",
    "\n",
    "tree_glob = './orthofinder/all_inputs/Results_Jul02/I2_rootknot_phylogenomics/'\n",
    "tree_glob += 'non_cds_alignment-trimal0.7-tree/RAxML_bestTree.284421467500872.59_*'\n",
    "\n",
    "aln_dir = './orthofinder/all_inputs/Results_Jul02/I2_rootknot_phylogenomics/'\n",
    "aln_dir += 'non_cds_alignment-trimal0.7/'\n",
    "\n",
    "I2_ad_hoc_exclude = [\n",
    "    # Short and orphan JB5_2 (no unconverted track)\n",
    "    # pulled to base\n",
    "    'OG0001486', 'OG0001835', 'OG0001937', 'OG0003836',\n",
    "    'OG0005058', 'OG0006313', 'OG0007028', \n",
    "    \n",
    "    # Short and orphan SJF1_2 (no unconverted track)\n",
    "    # pulled to base\n",
    "    'OG0003649', 'OG0002951', 'OG0005797',\n",
    "    \n",
    "    # Short and orphan SJF1_1 (no unconverted track)\n",
    "    # pulled to base\n",
    "     'OG0002772', 'OG0001804',\n",
    "    \n",
    "    # Short and orphan JB5_2 (no unconverted track)\n",
    "    # pulled to base\n",
    "    'OG0002588', 'OG0005562', 'OG0006349', \n",
    "    \n",
    "    # Paralogue pairs of MentL30\n",
    "    'OG0007678','OG0001804'\n",
    "\n",
    "] \n",
    "\n",
    "    \n",
    "for tname in glob.glob(tree_glob):\n",
    "    \n",
    "    OG = tname.split('_')[-1].split('.')[0]\n",
    "#    if OG in I2_ad_hoc_exclude:\n",
    "#        continue\n",
    "            \n",
    "    t = Tree(open(tname,'r').read())\n",
    "    t.dist = 0\n",
    "    t.ladderize()\n",
    "    \n",
    "    aln_file = \"%s%s.aln.trm.fasta\"%(aln_dir, '.'.join(tname.split('.')[-2:]))\n",
    "    \n",
    "    #if os.path.exists(alns_1_4+'/'+tname.rpartition('/')[-1]+'.1.fasta'):\n",
    "    #    continue\n",
    "    \n",
    "    ments = [l for l in t.get_leaves() if 'Ment' in l.name]\n",
    "    if len(ments) == 0:\n",
    "        continue\n",
    "    t.set_outgroup(ments[0])\n",
    "    \n",
    "    for l in t:\n",
    "        l.add_feature('source_organism', l.name.split('_')[0])\n",
    "    \n",
    "    for n in t.traverse():\n",
    "        N_sp_kids = len(set([l.source_organism for l in n.get_leaves()]))\n",
    "        if N_sp_kids > 1:\n",
    "            continue\n",
    "        rep_leaf = sorted(list(n.get_leaves()), key=lambda l: l.dist)[0]\n",
    "        n.add_feature('source_organism', rep_leaf.source_organism)\n",
    "        n.add_feature('feature_id', rep_leaf.name)\n",
    "        n.add_feature('representative','yes')\n",
    "        distance = rep_leaf.dist\n",
    "        newname  = rep_leaf.name\n",
    "        for c in n.get_children():\n",
    "            c.detach()\n",
    "        n.dist = n.dist+distance\n",
    "        n.name = newname\n",
    "        \n",
    "    all_samples = [l.source_organism for l in t.get_leaves()]\n",
    "    all_sample_leaves = t.get_leaves()\n",
    "    \n",
    "    ref_copy_counts = [Counter(all_samples)[a] for a in ['MincW1','MjavVW4','MentL30','MareHarA',\n",
    "                                                         'MfloSJF1']]\n",
    "    \n",
    "    all_ref_represented = all([i > 0 for i in ref_copy_counts])\n",
    "    \n",
    "    if not all_ref_represented:\n",
    "        continue\n",
    "        \n",
    "    # all pairwise distances\n",
    "    distances = []\n",
    "    for i,j in misc.iter_half_matrix_indices(all_sample_leaves):\n",
    "        distances.append(t.get_distance(all_sample_leaves[i],all_sample_leaves[j]))\n",
    "        \n",
    "    ogs = []\n",
    "    \n",
    "    # split leaves to two clusters\n",
    "    linkage = ward(distances)\n",
    "    assignments = hclust(linkage, 2)\n",
    "\n",
    "    clust1_samples = [all_samples[i] \n",
    "                      for i in range(len(assignments)) \n",
    "                      if assignments[i] == 1]\n",
    "    \n",
    "    clust2_samples = [all_samples[i] \n",
    "                      for i in range(len(assignments)) \n",
    "                      if assignments[i] == 2]\n",
    "    \n",
    "    this_is_two_ogs = 'MentL30' in clust1_samples and 'MentL30' in clust2_samples\n",
    "    \n",
    "    if this_is_two_ogs:\n",
    "        ogs.append([all_sample_leaves[i] for i in range(len(assignments))\n",
    "                    if assignments[i] == 1])\n",
    "        \n",
    "        ogs.append([all_sample_leaves[i] for i in range(len(assignments))\n",
    "                    if assignments[i] == 2])\n",
    "        \n",
    "    else:\n",
    "        ogs.append(t.get_leaves())\n",
    "        \n",
    "    n = 1    \n",
    "    for og in ogs:\n",
    "        og_sampls = [l.source_organism for l in og]\n",
    "        counts = [Counter(og_sampls)[a] for a in ['MincW1','MjavVW4','MentL30','MareHarA',\n",
    "                                                         'MfloSJF1']]\n",
    "        all1or2 = all([1 <= i <=2 for i in counts])\n",
    "        twoWithTwo = len([i for i in counts if i == 2]) >= 3\n",
    "        floWithTwo = Counter(og_sampls)['MfloSJF1'] == 2\n",
    "        allWithTwo = len([i for i in counts if i == 2]) == 5\n",
    "        \n",
    "        if not all1or2 or not twoWithTwo:\n",
    "            continue\n",
    "        \n",
    "        ingroup_leaves = [l for l in og if not 'MentL30' in l.source_organism]\n",
    "        ingroup_samples = [l.source_organism for l in ingroup_leaves]\n",
    "        \n",
    "        distances = []\n",
    "        for i,j in misc.iter_half_matrix_indices(ingroup_leaves):\n",
    "            distances.append(t.get_distance(ingroup_leaves[i],ingroup_leaves[j]))\n",
    "\n",
    "        linkage = ward(distances)\n",
    "\n",
    "        assignments = hclust(linkage, 2)\n",
    "        \n",
    "        clust1_samples = [ingroup_samples[i] \n",
    "                          for i in range(len(assignments)) \n",
    "                          if assignments[i] == 1]\n",
    "        \n",
    "        clust1_ids = [ingroup_leaves[i].name \n",
    "                      for i in range(len(assignments)) \n",
    "                      if assignments[i] == 1]\n",
    "        \n",
    "        good_clust1 = all([Counter(clust1_samples)[a] == 1 \n",
    "                           for a in clust1_samples])\n",
    "\n",
    "        clust2_samples = [ingroup_samples[i] \n",
    "                          for i in range(len(assignments)) \n",
    "                          if assignments[i] == 2]\n",
    "        \n",
    "        clust2_ids = [ingroup_leaves[i].name \n",
    "                      for i in range(len(assignments)) \n",
    "                      if assignments[i] == 2]\n",
    "        \n",
    "        good_clust2 =  all([Counter(clust2_samples)[a] == 1 \n",
    "                            for a in  clust2_samples])\n",
    "        \n",
    "        if not good_clust1 or not good_clust2:\n",
    "            \n",
    "            assignments = hclust(linkage, 3)\n",
    "            \n",
    "            clust1_samples = [ingroup_samples[i] \n",
    "                              for i in range(len(assignments)) \n",
    "                              if assignments[i] == 1]\n",
    "            \n",
    "            clust1_ids = [ingroup_leaves[i].name \n",
    "                          for i in range(len(assignments)) \n",
    "                          if assignments[i] == 1]\n",
    "            \n",
    "            clust2_samples = [ingroup_samples[i] \n",
    "                              for i in range(len(assignments)) \n",
    "                              if assignments[i] == 2]\n",
    "            \n",
    "            clust2_ids = [ingroup_leaves[i].name \n",
    "                          for i in range(len(assignments)) \n",
    "                          if assignments[i] == 2]\n",
    "            \n",
    "            clust3_samples = [ingroup_samples[i] \n",
    "                              for i in range(len(assignments)) \n",
    "                              if assignments[i] == 3]\n",
    "            \n",
    "            clust3_ids = [ingroup_leaves[i].name \n",
    "                          for i in range(len(assignments)) \n",
    "                          if assignments[i] == 3]\n",
    "\n",
    "            good = False\n",
    "            cluster1and2_samples = clust1_samples+clust2_samples\n",
    "            good_clust1and2 = all([Counter(cluster1and2_samples)[a] == 1 \n",
    "                                   for a in cluster1and2_samples])\n",
    "            \n",
    "            good_cluster3 = all([Counter(clust3_samples)[a] == 1 \n",
    "                                 for a in  clust3_samples])\n",
    "            \n",
    "            if good_clust1and2 and good_cluster3:\n",
    "                clust1_ids = clust1_ids + clust2_ids\n",
    "                clust2_ids = clust3_ids\n",
    "                good = True\n",
    "\n",
    "            cluster1and3_samples = clust1_samples+clust3_samples\n",
    "            good_clust1and3 = all([Counter(cluster1and3_samples)[a] == 1 \n",
    "                                   for a in  cluster1and3_samples])\n",
    "            \n",
    "            good_cluster2 = all([Counter(clust2_samples)[a] == 1 \n",
    "                                 for a in  clust2_samples])\n",
    "            \n",
    "            if good_clust1and3 and good_cluster2 and not good:\n",
    "                clust1_ids = clust1_ids + clust3_ids\n",
    "                clust2_ids = clust2_ids\n",
    "                good = True\n",
    "\n",
    "            cluster2and3_samples = clust2_samples+clust3_samples\n",
    "            good_clust2and3 = all([Counter(cluster2and3_samples)[a] == 1 for a in  cluster2and3_samples])\n",
    "            good_cluster1 = all([Counter(clust1_samples)[a] == 1 for a in clust1_samples])\n",
    "            if good_clust2and3 and good_cluster1 and not good:\n",
    "                clust2_ids = clust2_ids + clust3_ids\n",
    "                clust1_ids = clust1_ids\n",
    "                good = True\n",
    "                \n",
    "            if not good:\n",
    "                continue\n",
    "                \n",
    "        ment_homeoplogues = ['1','2','3','4']\n",
    "        for l in og:\n",
    "            if l.name in clust1_ids:\n",
    "                l.add_feature('Homeologue','1')\n",
    "                pj.add_qualifier([l.name],'Homeologue', '1')\n",
    "            elif l.name in clust2_ids:\n",
    "                l.add_feature('Homeologue','2')\n",
    "                pj.add_qualifier([l.name],'Homeologue', '2')\n",
    "            elif 'Ment' in l.source_organism:\n",
    "                hom = ment_homeoplogues.pop(0)\n",
    "                l.add_feature('Homeologue',hom)\n",
    "                pj.add_qualifier([l.name],'Homeologue', hom)\n",
    "\n",
    "\n",
    "        with open(alns_1_4+'/'+tname.rpartition('/')[-1]+'.%i.fasta'%n,'wt') as hndl:\n",
    "            for r in AlignIO.read(aln_file,'fasta'):\n",
    "                if r.id in [l.name for l in og]:\n",
    "                    quals = get_qualifiers_dictionary(pj, r.id)\n",
    "                    r.id = quals['source_organism'] + '_'\n",
    "                    r.id += quals['Homeologue']\n",
    "                    hndl.write(r.format('fasta'))\n",
    "        \n",
    "        if floWithTwo:\n",
    "            with open(alns_flo2+'/'+tname.rpartition('/')[-1]+'.%i.fasta'%n,'wt') as hndl:\n",
    "                for r in AlignIO.read(aln_file,'fasta'):\n",
    "                    if r.id in [l.name for l in og]:\n",
    "                        quals = get_qualifiers_dictionary(pj, r.id)\n",
    "                        r.id = quals['source_organism'] + '_'\n",
    "                        r.id += quals['Homeologue']\n",
    "                        hndl.write(r.format('fasta'))\n",
    "                        \n",
    "        if allWithTwo:\n",
    "            with open(alns_all2+'/'+tname.rpartition('/')[-1]+'_%i.fasta'%n,'wt') as hndl:\n",
    "                for r in AlignIO.read(aln_file,'fasta'):\n",
    "                    if r.id in [l.name for l in og]:\n",
    "                        quals = get_qualifiers_dictionary(pj, r.id)\n",
    "                        r.id = quals['source_organism'] + '_'\n",
    "                        r.id += quals['Homeologue']\n",
    "                        hndl.write(r.format('fasta'))\n",
    "                      \n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.7 Collapse inparalogues and write fasta files based on the strict trimal settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reprophylodev import *\n",
    "from collections import Counter\n",
    "import pickle, misc, os, pickle\n",
    "from fastcluster import ward\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "def hclust(linkmat, nclusters):\n",
    "    \"\"\"\n",
    "    based on the treeCl _hclust function\n",
    "    \"\"\"\n",
    "    linkmat_size = len(linkmat)\n",
    "    if nclusters <= 1:\n",
    "        br_top = linkmat[linkmat_size - nclusters][2]\n",
    "    else:\n",
    "        br_top = linkmat[linkmat_size - nclusters + 1][2]\n",
    "    if nclusters >= len(linkmat):\n",
    "        br_bottom = 0\n",
    "    else:\n",
    "        br_bottom = linkmat[linkmat_size - nclusters][2]\n",
    "    threshold = 0.5 * (br_top + br_bottom)\n",
    "    assignments = fcluster(linkmat, threshold, criterion='distance')\n",
    "    return assignments\n",
    "\n",
    "base += './orthofinder/all_inputs/Results_Jul02/'\n",
    "\n",
    "alns_1_4 = base+'I2_2X2_gt0.85_st0.005_alns_1_4'\n",
    "alns_flo2 = base+'I2_2X2_gt0.85_st0.005_alns_flo2'\n",
    "alns_all2 = base+'I2_2X2_gt0.85_st0.005_alns_all2'\n",
    "gb_out = 'orthofinder/all_inputs/Results_Jul02/OGs_I2_1-4.gb'\n",
    "\n",
    "for alndir in [alns_1_4,alns_flo2,alns_all2]:\n",
    "    misc.makedir(alndir)\n",
    "\n",
    "with open(gb_out+'.pkpj','rb') as hndl:\n",
    "    pj = pickle.load(hndl)\n",
    "\n",
    "tree_glob = './orthofinder/all_inputs/Results_Jul02/I2_2X2_trimal0.85_rootknot_phylogenomics/'\n",
    "tree_glob += 'non_cds_alignment-trimal0.7-tree/RAxML_bestTree.284421467500872.59_*'\n",
    "\n",
    "aln_dir = './orthofinder/all_inputs/Results_Jul02/I2_2X2_trimal0.85_rootknot_phylogenomics/'\n",
    "aln_dir += 'non_cds_alignment-trimal0.7/'\n",
    "\n",
    "I2_ad_hoc_exclude = [\n",
    "    'OG0003649','OG0002951','OG0005797','OG0002772','OG0001804',\n",
    "    'OG0001486','OG0001835','OG0001937','OG0003836','OG0006313',\n",
    "    'OG0007028','OG0002588','OG0005562','OG006349','OG0007678',\n",
    "\n",
    "\n",
    "] \n",
    "\n",
    "\n",
    "for tname in glob.glob(tree_glob):\n",
    "\n",
    "    OG = tname.split('_')[-1].split('.')[0]\n",
    "    if OG in I2_ad_hoc_exclude:\n",
    "        continue\n",
    "\n",
    "    t = Tree(open(tname,'r').read())\n",
    "    t.dist = 0\n",
    "    t.ladderize()\n",
    "\n",
    "    aln_file = \"%s%s.aln.trm.fasta\"%(aln_dir, '.'.join(tname.split('.')[-2:]))\n",
    "\n",
    "    #if os.path.exists(alns_1_4+'/'+tname.rpartition('/')[-1]+'.1.fasta'):\n",
    "    #    continue\n",
    "\n",
    "    ments = [l for l in t.get_leaves() if 'Ment' in l.name]\n",
    "    if len(ments) == 0:\n",
    "        continue\n",
    "    t.set_outgroup(ments[0])\n",
    "\n",
    "    for l in t:\n",
    "        l.add_feature('source_organism', l.name.split('_')[0])\n",
    "\n",
    "    for n in t.traverse():\n",
    "        N_sp_kids = len(set([l.source_organism for l in n.get_leaves()]))\n",
    "        if N_sp_kids > 1:\n",
    "            continue\n",
    "        rep_leaf = sorted(list(n.get_leaves()), key=lambda l: l.dist)[0]\n",
    "        n.add_feature('source_organism', rep_leaf.source_organism)\n",
    "        n.add_feature('feature_id', rep_leaf.name)\n",
    "        n.add_feature('representative','yes')\n",
    "        distance = rep_leaf.dist\n",
    "        newname  = rep_leaf.name\n",
    "        for c in n.get_children():\n",
    "            c.detach()\n",
    "        n.dist = n.dist+distance\n",
    "        n.name = newname\n",
    "\n",
    "    all_samples = [l.source_organism for l in t.get_leaves()]\n",
    "    all_sample_leaves = t.get_leaves()\n",
    "\n",
    "    ref_copy_counts = [Counter(all_samples)[a] for a in ['MincW1','MjavVW4','MentL30','MareHarA',\n",
    "                                                         'MfloSJF1']]\n",
    "\n",
    "    all_ref_represented = all([i > 0 for i in ref_copy_counts])\n",
    "\n",
    "    if not all_ref_represented:\n",
    "        continue\n",
    "\n",
    "    # all pairwise distances\n",
    "    distances = []\n",
    "    for i,j in misc.iter_half_matrix_indices(all_sample_leaves):\n",
    "        distances.append(t.get_distance(all_sample_leaves[i],all_sample_leaves[j]))\n",
    "\n",
    "    ogs = []\n",
    "\n",
    "    # split leaves to two clusters\n",
    "    linkage = ward(distances)\n",
    "    assignments = hclust(linkage, 2)\n",
    "\n",
    "    clust1_samples = [all_samples[i] \n",
    "                      for i in range(len(assignments)) \n",
    "                      if assignments[i] == 1]\n",
    "\n",
    "    clust2_samples = [all_samples[i] \n",
    "                      for i in range(len(assignments)) \n",
    "                      if assignments[i] == 2]\n",
    "\n",
    "    this_is_two_ogs = 'MentL30' in clust1_samples and 'MentL30' in clust2_samples\n",
    "\n",
    "    if this_is_two_ogs:\n",
    "        ogs.append([all_sample_leaves[i] for i in range(len(assignments))\n",
    "                    if assignments[i] == 1])\n",
    "\n",
    "        ogs.append([all_sample_leaves[i] for i in range(len(assignments))\n",
    "                    if assignments[i] == 2])\n",
    "\n",
    "    else:\n",
    "        ogs.append(t.get_leaves())\n",
    "\n",
    "    n = 1    \n",
    "    for og in ogs:\n",
    "        og_sampls = [l.source_organism for l in og]\n",
    "        counts = [Counter(og_sampls)[a] for a in ['MincW1','MjavVW4','MentL30','MareHarA',\n",
    "                                                         'MfloSJF1']]\n",
    "        all1or2 = all([1 <= i <=2 for i in counts])\n",
    "        twoWithTwo = len([i for i in counts if i == 2]) >= 2\n",
    "        floWithTwo = Counter(og_sampls)['MfloSJF1'] == 2\n",
    "        allWithTwo = len([i for i in counts if i == 2]) == 5\n",
    "\n",
    "        if not all1or2 or not twoWithTwo:\n",
    "            continue\n",
    "\n",
    "        ingroup_leaves = [l for l in og if not 'MentL30' in l.source_organism]\n",
    "        ingroup_samples = [l.source_organism for l in ingroup_leaves]\n",
    "\n",
    "        distances = []\n",
    "        for i,j in misc.iter_half_matrix_indices(ingroup_leaves):\n",
    "            distances.append(t.get_distance(ingroup_leaves[i],ingroup_leaves[j]))\n",
    "\n",
    "        linkage = ward(distances)\n",
    "\n",
    "        assignments = hclust(linkage, 2)\n",
    "\n",
    "        clust1_samples = [ingroup_samples[i] \n",
    "                          for i in range(len(assignments)) \n",
    "                          if assignments[i] == 1]\n",
    "\n",
    "        clust1_ids = [ingroup_leaves[i].name \n",
    "                      for i in range(len(assignments)) \n",
    "                      if assignments[i] == 1]\n",
    "\n",
    "        good_clust1 = all([Counter(clust1_samples)[a] == 1 \n",
    "                           for a in clust1_samples])\n",
    "\n",
    "        clust2_samples = [ingroup_samples[i] \n",
    "                          for i in range(len(assignments)) \n",
    "                          if assignments[i] == 2]\n",
    "\n",
    "        clust2_ids = [ingroup_leaves[i].name \n",
    "                      for i in range(len(assignments)) \n",
    "                      if assignments[i] == 2]\n",
    "\n",
    "        good_clust2 =  all([Counter(clust2_samples)[a] == 1 \n",
    "                            for a in  clust2_samples])\n",
    "\n",
    "        if not good_clust1 or not good_clust2:\n",
    "\n",
    "            assignments = hclust(linkage, 3)\n",
    "\n",
    "            clust1_samples = [ingroup_samples[i] \n",
    "                              for i in range(len(assignments)) \n",
    "                              if assignments[i] == 1]\n",
    "\n",
    "            clust1_ids = [ingroup_leaves[i].name \n",
    "                          for i in range(len(assignments)) \n",
    "                          if assignments[i] == 1]\n",
    "\n",
    "            clust2_samples = [ingroup_samples[i] \n",
    "                              for i in range(len(assignments)) \n",
    "                              if assignments[i] == 2]\n",
    "\n",
    "            clust2_ids = [ingroup_leaves[i].name \n",
    "                          for i in range(len(assignments)) \n",
    "                          if assignments[i] == 2]\n",
    "\n",
    "            clust3_samples = [ingroup_samples[i] \n",
    "                              for i in range(len(assignments)) \n",
    "                              if assignments[i] == 3]\n",
    "\n",
    "            clust3_ids = [ingroup_leaves[i].name \n",
    "                          for i in range(len(assignments)) \n",
    "                          if assignments[i] == 3]\n",
    "\n",
    "            good = False\n",
    "            cluster1and2_samples = clust1_samples+clust2_samples\n",
    "            good_clust1and2 = all([Counter(cluster1and2_samples)[a] == 1 \n",
    "                                   for a in cluster1and2_samples])\n",
    "\n",
    "            good_cluster3 = all([Counter(clust3_samples)[a] == 1 \n",
    "                                 for a in  clust3_samples])\n",
    "\n",
    "            if good_clust1and2 and good_cluster3:\n",
    "                clust1_ids = clust1_ids + clust2_ids\n",
    "                clust2_ids = clust3_ids\n",
    "                good = True\n",
    "\n",
    "            cluster1and3_samples = clust1_samples+clust3_samples\n",
    "            good_clust1and3 = all([Counter(cluster1and3_samples)[a] == 1 \n",
    "                                   for a in  cluster1and3_samples])\n",
    "\n",
    "            good_cluster2 = all([Counter(clust2_samples)[a] == 1 \n",
    "                                 for a in  clust2_samples])\n",
    "\n",
    "            if good_clust1and3 and good_cluster2 and not good:\n",
    "                clust1_ids = clust1_ids + clust3_ids\n",
    "                clust2_ids = clust2_ids\n",
    "                good = True\n",
    "\n",
    "            cluster2and3_samples = clust2_samples+clust3_samples\n",
    "            good_clust2and3 = all([Counter(cluster2and3_samples)[a] == 1 for a in  cluster2and3_samples])\n",
    "            good_cluster1 = all([Counter(clust1_samples)[a] == 1 for a in clust1_samples])\n",
    "            if good_clust2and3 and good_cluster1 and not good:\n",
    "                clust2_ids = clust2_ids + clust3_ids\n",
    "                clust1_ids = clust1_ids\n",
    "                good = True\n",
    "\n",
    "            if not good:\n",
    "                continue\n",
    "\n",
    "        ment_homeoplogues = ['1','2','3','4']\n",
    "        for l in og:\n",
    "            if l.name in clust1_ids:\n",
    "                l.add_feature('Homeologue','1')\n",
    "                pj.add_qualifier([l.name],'Homeologue', '1')\n",
    "            elif l.name in clust2_ids:\n",
    "                l.add_feature('Homeologue','2')\n",
    "                pj.add_qualifier([l.name],'Homeologue', '2')\n",
    "            elif 'Ment' in l.source_organism:\n",
    "                hom = ment_homeoplogues.pop(0)\n",
    "                l.add_feature('Homeologue',hom)\n",
    "                pj.add_qualifier([l.name],'Homeologue', hom)\n",
    "\n",
    "\n",
    "        with open(alns_1_4+'/'+tname.rpartition('/')[-1]+'.%i.fasta'%n,'wt') as hndl:\n",
    "            for r in AlignIO.read(aln_file,'fasta'):\n",
    "                if r.id in [l.name for l in og]:\n",
    "                    quals = get_qualifiers_dictionary(pj, r.id)\n",
    "                    r.id = quals['source_organism'] + '_'\n",
    "                    r.id += quals['Homeologue']\n",
    "                    hndl.write(r.format('fasta'))\n",
    "\n",
    "        if floWithTwo:\n",
    "            with open(alns_flo2+'/'+tname.rpartition('/')[-1]+'.%i.fasta'%n,'wt') as hndl:\n",
    "                for r in AlignIO.read(aln_file,'fasta'):\n",
    "                    if r.id in [l.name for l in og]:\n",
    "                        quals = get_qualifiers_dictionary(pj, r.id)\n",
    "                        r.id = quals['source_organism'] + '_'\n",
    "                        r.id += quals['Homeologue']\n",
    "                        hndl.write(r.format('fasta'))\n",
    "\n",
    "        if allWithTwo:\n",
    "            with open(alns_all2+'/'+tname.rpartition('/')[-1]+'_%i.fasta'%n,'wt') as hndl:\n",
    "                for r in AlignIO.read(aln_file,'fasta'):\n",
    "                    if r.id in [l.name for l in og]:\n",
    "                        quals = get_qualifiers_dictionary(pj, r.id)\n",
    "                        r.id = quals['source_organism'] + '_'\n",
    "                        r.id += quals['Homeologue']\n",
    "                        hndl.write(r.format('fasta'))\n",
    "\n",
    "        n += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
