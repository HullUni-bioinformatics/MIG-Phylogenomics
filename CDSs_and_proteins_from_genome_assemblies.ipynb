{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get genes from reference genomes of Mflo, W1, Mjav, Mare, Mkon and Ment\n",
    "Using the genome assemblies and their gff files, make a file of genes (introns included), to be used as mitobim baits for the Mflo read, the reads from the W1 mitochondrial clade, the Mjav mitochondtial clade and Mare respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Annotation files\n",
    "Only gff3 can go into this dictionary. On occassions where gff2 is available from exonerate, it is dealt with locally with an addapted script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These paths will not work on BioPC8\n",
    "# which has a unique file structure\n",
    "\n",
    "root = './annotation/'\n",
    "\n",
    "\n",
    "mflo_gff = None\n",
    "w1_gff = root + 'mincognita.augustus.gff3'\n",
    "vw4_gff = root + 'mjavanicaVW4.augustus.gff3'\n",
    "Mare_gff = root + 'marenaria.augustus.gff3'\n",
    "Mkon_gff = None\n",
    "Ment_gff = root + 'menterolobii.augustus.gff3'\n",
    "MfloSJF1_gff = root + 'mfloridensis.augustus.gff3'\n",
    "\n",
    "annotation_files = {'MfloJB5_ref':mflo_gff,\n",
    "                    'MfloSJF1_ref': MfloSJF1_gff,\n",
    "                    'MjavVW4_ref':vw4_gff,\n",
    "                    'MincW1_ref':w1_gff,\n",
    "                    'MareHarA_ref':Mare_gff,\n",
    "                    #'Mkon_ref':Mkon_gff,\n",
    "                    'MentL30_ref':Ment_gff}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Genome assembly files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!gzip -d ./meloidogyne_assemblies/*.fa*gz\n",
    "\n",
    "root = './meloidogyne_assemblies/'\n",
    "\n",
    "mflo_ass = './meloidogyne_assemblies/meloidogyne.velvet.fa'\n",
    "w1_ass = './meloidogyne_assemblies/mincognita_gapClosed.fa'\n",
    "vw4_ass = './meloidogyne_assemblies/mjavanica_gapClosed.fa'\n",
    "Mare_ass = './meloidogyne_assemblies/HarA_gapClosed.fa'\n",
    "#Mkon_ass = './meloidogyne_assemblies/Kon_gapClosed.fa'\n",
    "Ment_ass = \"/media/amir/DATA/Read_data/assemblies/Ent_gapClosed.fa\"\n",
    "MfloSJF1_ass = root+\"MfloSJ1.scf.fasta\"\n",
    "    \n",
    "assembly_files = {'MfloJB5_ref':mflo_ass,\n",
    "                  'MfloSJF1_ref': MfloSJF1_ass,\n",
    "                  'MjavVW4_ref':vw4_ass,\n",
    "                  'MincW1_ref':w1_ass,\n",
    "                  'MareHarA_ref':Mare_ass,\n",
    "                  #'Mkon_ref': Mkon_ass,\n",
    "                  'MentL30_ref': Ment_ass}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Write gene, CDS and Prot fasta files\n",
    "### 1.3.1 collect the sequences from the assembly and write to fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from misc import *\n",
    "import os, gc, sys\n",
    "\n",
    "from Bio import SeqIO\n",
    "# make output directories\n",
    "for fpath in ['protein_ref_file',\n",
    "              'cds_ref_files',\n",
    "              'gene_ref_files']:\n",
    "    makedir(fpath)\n",
    "    \n",
    "for ref in annotation_files:\n",
    "    annotation = annotation_files[ref]\n",
    "    assembly = assembly_files[ref]\n",
    "    \n",
    "    # annotation and assembly exist?\n",
    "    if annotation and assembly:\n",
    "        \n",
    "        prot_out = 'protein_ref_file/'+ref+'.aa.fasta'\n",
    "        cds_out = 'cds_ref_files/'+ref+'.cds.fasta'\n",
    "        gene_out = 'gene_ref_files/'+ref+'.nt.fasta'\n",
    "        \n",
    "        # output files exists?\n",
    "        prot_exists = os.path.exists(prot_out)\n",
    "        cds_exists = os.path.exists(cds_out)\n",
    "        gene_exists = os.path.exists(gene_out)\n",
    "        \n",
    "        # Do we need to make more output files?\n",
    "        if all([prot_exists,cds_exists,gene_exists]):\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        gene_hndl = None\n",
    "        cds_hndl = None\n",
    "        prot_hndl = None\n",
    "        \n",
    "        if not gene_exists:\n",
    "            gene_hndl = open(gene_out,'wt')\n",
    "        \n",
    "        if not cds_exists:\n",
    "            cds_hndl = open(cds_out,'wt')\n",
    "        \n",
    "        if not prot_exists:\n",
    "            prot_hndl = open(prot_out,'wt')\n",
    "            \n",
    "        records = SeqIO.to_dict(SeqIO.parse(assembly,'fasta'))\n",
    "        \n",
    "        annotation_hndl = open(annotation,'r')\n",
    "        \n",
    "        gene_id = None\n",
    "        cds_seq = None\n",
    "        gene_seq = None\n",
    "        \n",
    "        for gff in annotation_hndl:\n",
    "            contig, a, featuretype, start, end, b, strand, c, qualifiers = gff.split('\\t')\n",
    "                \n",
    "            start = int(start)\n",
    "            end = int(end)\n",
    "\n",
    "            qualifiers = {q.split('=')[0]: q.split('=')[1] for\n",
    "                              q in qualifiers.rstrip().split(';') if q}\n",
    "                \n",
    "            start -= 1\n",
    "            \n",
    "            if not 'ID' in qualifiers:\n",
    "                continue\n",
    "                \n",
    "            if not gene_id and featuretype == 'gene':\n",
    "                gene_id = qualifiers[\"ID\"]\n",
    "                gene_seq = records[contig].seq[start:end]\n",
    "                if strand == '-':\n",
    "                    gene_seq = gene_seq.reverse_complement()\n",
    "                gene_hndl.write('>%s\\n%s\\n'%(gene_id, str(gene_seq)))\n",
    "            \n",
    "            elif (gene_id and \n",
    "                not gene_id == qualifiers['ID'].split('.')[0] and \n",
    "                featuretype == 'gene'):\n",
    "                \n",
    "                cds_hndl.write('>%s\\n%s\\n'%(gene_id, str(cds_seq)))\n",
    "                prot_seq = str(cds_seq.translate())\n",
    "                if prot_seq.endswith('*'):\n",
    "                    prot_seq = prot_seq[:-1]\n",
    "                prot_hndl.write('>%s\\n%s\\n'%(gene_id, prot_seq))\n",
    "                \n",
    "                cds_seq = None\n",
    "                prot_seq = None\n",
    "                \n",
    "                gene_id = qualifiers[\"ID\"]\n",
    "                gene_seq = records[contig].seq[start:end]\n",
    "                if strand == '-':\n",
    "                    gene_seq = gene_seq.reverse_complement()\n",
    "                gene_hndl.write('>%s\\n%s\\n'%(gene_id, str(gene_seq)))\n",
    "                \n",
    "            elif featuretype == 'CDS' and qualifiers['ID'].split('.')[0] == gene_id:\n",
    "                if not cds_seq:\n",
    "                    cds_seq = ''\n",
    "                exon_seq = records[contig].seq[start:end]\n",
    "                if strand == '+':\n",
    "                    cds_seq += exon_seq\n",
    "                elif strand == '-':\n",
    "                    cds_seq = exon_seq.reverse_complement() + cds_seq\n",
    "                else:\n",
    "                    raise RuntimeError('dont know strand %s'%strand)\n",
    "        if cds_seq:\n",
    "            cds_hndl.write('>%s\\n%s\\n'%(gene_id, str(cds_seq)))\n",
    "            prot_seq = str(cds_seq.translate())\n",
    "            if prot_seq.endswith('*'):\n",
    "                prot_seq = prot_seq[:-1]\n",
    "            prot_hndl.write('>%s\\n%s\\n'%(gene_id, prot_seq))\n",
    "            \n",
    "                \n",
    "   \n",
    "        \n",
    "        # close the output files\n",
    "        \n",
    "        annotation_hndl.close()\n",
    "                                    \n",
    "        if not gene_hndl:\n",
    "            gene_hndl.close()\n",
    "        \n",
    "        if not cds_hndl:\n",
    "            cds_hndl.close()\n",
    "        \n",
    "        if not prot_hndl:\n",
    "            prot_hndl.close()\n",
    "                    \n",
    "        # clear the memory\n",
    "        del records\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Take a different approach with Mflo which is gff2\n",
    "The gff2 for Mflo was made by running exonerate with cdss as queries. It was done for the mapping analysis. Since the CDS and protein files already exist from Lunt et al 2014, and these CDSs were used to make the gff2 to beginwith, there is no need to extract proteins and cdss again, only genes, from the gff2.\n",
    "### 1.4.1 Copy the Mflo CDS, protein and gff files from Lunt et al. 2014:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# The copied files are already in the destination inside this repository\n",
    "\n",
    "if not os.path.exists('cds_ref_files/MfloJB5_ref.cds.fasta'):\n",
    "    !cp ../GS978784/mf.cds.fna cds_ref_files/MfloJB5_ref.cds.fasta\n",
    "if not os.path.exists('protein_ref_file/MfloJB5_ref.aa.fasta'):\n",
    "    !cp ../GS978784/mf.protein.faa protein_ref_file/MfloJB5_ref.aa.fasta\n",
    "if not os.path.exists('annotations/MfloJB5.gff'):\n",
    "    !cp ../GS978784/mf.cds.fna annotation/MfloJB5.gff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Extract full genes from the MfloJB5 assembly using the gff2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from Bio import SeqIO\n",
    "\n",
    "mflo_gff2 = 'annotation/MfloJB5.gff'\n",
    "\n",
    "# mflo assembly:\n",
    "records = SeqIO.to_dict(SeqIO.parse(assembly_files['MfloJB5_ref'],'fasta'))\n",
    "\n",
    "mflo_genes_fpath = 'gene_ref_files/MfloJB5_ref.nt.fasta'\n",
    "\n",
    "if not os.path.exists(mflo_genes_fpath):\n",
    "\n",
    "    hndl = open(mflo_genes_fpath,'wt')\n",
    "\n",
    "    with open(mflo_gff2,'r') as lines:\n",
    "        for line in lines:\n",
    "            contig, a, featuretype, start, end, b, strand, c, qualifiers = line.split('\\t')\n",
    "            if featuretype == 'gene':\n",
    "                start = int(start)\n",
    "                end = int(end)\n",
    "                \n",
    "                # gff2 style\n",
    "                qualifiers = {q.split()[0]: q.split()[1] for\n",
    "                              q in qualifiers.rstrip().split(' ; ') if q}\n",
    "                start -= 1\n",
    "                \n",
    "                gene_id = qualifiers['sequence']\n",
    "                gene_seq = records[contig].seq[start:end]\n",
    "                if strand == '-':\n",
    "                    gene_seq = gene_seq.reverse_complement()\n",
    "                hndl.write('>%s\\n%s\\n'%(gene_id, str(gene_seq)))\n",
    "\n",
    "\n",
    "    hndl.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Annotate unannotated genomes with exonerate \n",
    "All the denovo assembled genomes (ie reference genomes) now have an augustus gff3 annotation and section 1.5 is obsolete. The functions used here may still be usfull at some point so I keep it in the notebook.\n",
    "\n",
    "### 1.5.1 Prerequisites to run and  parse exonerate output\n",
    "These will run a multithread exonerate analysis using the protein2genome model and will parse the spesific output into a cds file, a gene file and a gff2 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from misc import *\n",
    "from StringIO import StringIO\n",
    "\n",
    "\n",
    "def run_exonerate(q, t, outdir, threads):\n",
    "    \n",
    "    import time, threading\n",
    "    from glob import glob\n",
    "    from warnings import warn\n",
    "    from subprocess import Popen\n",
    "\n",
    "    makedir(outdir)\n",
    "\n",
    "    # The exonerate command line\n",
    "\n",
    "    # chunk the query file\n",
    "    a = \"exonerate --querychunkid %i --querychunktotal %i \"\n",
    "    # format the output into gene and CDS fasta (exonerate roll your own syntax\n",
    "    # plus proper Python escapes: backslash for quotation and backslash,\n",
    "    # percent for percent)\n",
    "    b = \"--ryo \\\"STARTGENE\\\\n>%%qi\\\\n%%tas\\\\nENDGENE\\\\nSTARTCDS\\\\n>%%qi\\\\n%%tcs\\\\nENDCDS \\\" \"\n",
    "    # gff\n",
    "    c = \"--showtargetgff \"\n",
    "    # model and limit to number of matches\n",
    "    d = \"-m protein2genome \"#-n 1 \"\n",
    "    # query and target file paths\n",
    "    e = \"-q %s -t %s \"%(q,t)\n",
    "    # output fpath\n",
    "    f = \"> %s\"%outdir+\"/%s\"\n",
    "\n",
    "    exonerate_cline = a+b+c+d+e+f\n",
    "\n",
    "    class MyThread(threading.Thread):\n",
    "        def __init__(self, querychunkid, querychunktotal, out):\n",
    "            threading.Thread.__init__(self)\n",
    "            self.querychunkid = querychunkid\n",
    "            self.querychunktotal = querychunktotal\n",
    "            self.out = out\n",
    "\n",
    "        def run(self):\n",
    "            cline = exonerate_cline%(self.querychunkid,\n",
    "                                     self.querychunktotal,\n",
    "                                     self.out)\n",
    "            p = Popen(cline,shell=True)\n",
    "            p.communicate()\n",
    "\n",
    "\n",
    "    # outputs exist?\n",
    "    outputs_exist = len(list(glob('%s/exoout*'%outdir))) > 0\n",
    "\n",
    "    if outputs_exist:\n",
    "        warn('some output exists. Skipping')\n",
    "    else:\n",
    "        threads = [MyThread(i, threads, 'exoout%i'%i) for i in range(1, threads+1)]\n",
    "\n",
    "        for t in threads:\n",
    "            t.start()\n",
    "\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "            \n",
    "        print \"exonerate done\"\n",
    "\n",
    "\n",
    "def parse_a_single_record(record):\n",
    "    \n",
    "    output = {}\n",
    "    \n",
    "    def parse_line_element(ind, parter, element_name):\n",
    "        elementline = record[ind]\n",
    "        try:\n",
    "            output[element_name] = elementline.rstrip().split(parter)[1]\n",
    "            if ' [revcomp]' in elementline:\n",
    "                output[element_name] = output[element_name].replace(' [revcomp]','')\n",
    "            elif ' -> ' in output[element_name] and \\\n",
    "                 element_name == 'Target_start':\n",
    "                output[element_name] = output[element_name].split(' -> ')[0]\n",
    "            elif ' -> ' in output[element_name] and \\\n",
    "                 element_name == 'Target_end':\n",
    "                output[element_name] = output[element_name].split(' -> ')[1]\n",
    "            try:\n",
    "                output[element_name] = int(output[element_name])\n",
    "            except:\n",
    "                pass\n",
    "        except:\n",
    "            raise RuntimeError(\"%s not found\"%parter)\n",
    "\n",
    "            \n",
    "    line_elements = [[2, 'Query: ', 'Query'],\n",
    "                     [3, 'Target: ', 'Target'],\n",
    "                     [5, 'Raw score: ', 'Score'],\n",
    "                     [7, 'Target range: ', 'Target_start'],\n",
    "                     [7, 'Target range: ', 'Target_end']]\n",
    "    \n",
    "    \n",
    "    for ind, parter, element_name in line_elements:\n",
    "        parse_line_element(ind, parter, element_name)\n",
    "        \n",
    "    def parse_block_element(init, halt, element_name):\n",
    "        element = \"\"\n",
    "        get = False\n",
    "        for line in record:\n",
    "            if init in line:\n",
    "                get = True\n",
    "            elif halt in line:\n",
    "                get = False\n",
    "            elif get:\n",
    "                element += line\n",
    "        output[element_name] = element\n",
    "        \n",
    "    block_elements = [['STARTGENE','ENDGENE','Gene'],\n",
    "                      ['STARTCDS','ENDCDS','CDS'],\n",
    "                      ['--- START OF GFF DUMP ---',\n",
    "                       '--- END OF GFF DUMP ---',\n",
    "                       'GFF']\n",
    "                     ]\n",
    "    for init, halt, element_name in block_elements:\n",
    "        parse_block_element(init, halt, element_name)\n",
    "        \n",
    "    return output\n",
    "\n",
    "def records_overlapping(record1,record2):\n",
    "    \n",
    "    from misc import is_overlapping\n",
    "    \n",
    "    contig1 = record1['Target']\n",
    "    contig2 = record2['Target']\n",
    "    if not contig1 == contig2:\n",
    "        return False, None\n",
    "    \n",
    "    coords1 = (record1['Target_start'], record1['Target_end'])\n",
    "    coords2 = (record2['Target_start'], record2['Target_end'])\n",
    "    \n",
    "    if not is_overlapping(coords1, coords2):\n",
    "        return False, None\n",
    "    \n",
    "    if record1['Score'] >= record2['Score']:\n",
    "        return True, 0\n",
    "    else:\n",
    "        return True, 1\n",
    "    \n",
    "def iter_records(fpath):\n",
    "    with open(fpath,'r') as lines:\n",
    "        record = []\n",
    "        get = False\n",
    "        for line in lines:\n",
    "            if 'C4 Alignment:' in line:\n",
    "                get = True\n",
    "            if get and 'C4 Alignment:' in line:\n",
    "                if len(record) > 0:\n",
    "                    yield record\n",
    "                    record = [line]\n",
    "                else:\n",
    "                    record = [line]\n",
    "            elif get:\n",
    "                record.append(line)\n",
    "        yield record\n",
    "        \n",
    "def filter_out_short_exonerate_matches(exonerate_records_with_CDS, query_lengths, len_cutoff=0.95, n_cutoff = 0.1):\n",
    "    \"\"\"\n",
    "    exonerate_records_with_CDS is a list of exonerate records \n",
    "    each parsed with parse_a_single_record.\n",
    "    Individual records can be passed to parse_a_single_record\n",
    "    from an exonerate output file with iter_records\n",
    "    The output file has to come from an exonerate run in which\n",
    "    the following ryo was used:\n",
    "    --ryo \\\"STARTGENE\\\\n>%%qi\\\\n%%tas\\\\nENDGENE\\\\nSTARTCDS\\\\n>%%qi\\\\n%%tcs\\\\nENDCDS \\\"\n",
    "    (the first \\ or % are python excapes and should be removed in the command line)\n",
    "    \"\"\"\n",
    "    records_to_remove = set()\n",
    "    for i in range(len(exonerate_records_with_CDS)):\n",
    "        cdsfastahndl = StringIO(exonerate_records_with_CDS[i]['CDS'])\n",
    "        seqrecord = SeqIO.read(cdsfastahndl,'fasta')\n",
    "        length = len(seqrecord.seq)\n",
    "        if length < query_lengths[exonerate_records_with_CDS[i]['Query']]*len_cutoff:\n",
    "            records_to_remove.add(i)\n",
    "        elif str(seqrecord.seq).count('N') > query_lengths[exonerate_records_with_CDS[i]['Query']]*n_cutoff:\n",
    "            records_to_remove.add(i)\n",
    "    records_to_remove = sorted(records_to_remove, reverse=True)\n",
    "    for i in records_to_remove:\n",
    "        exonerate_records_with_CDS.pop(i)\n",
    "    return exonerate_records_with_CDS\n",
    "        \n",
    "def iter_half_matrix_indices(iterable):\n",
    "    \"\"\"\n",
    "    >>> iterable = [1,2,3]\n",
    "    >>> for i,j in iter_half_matrix_indices(iterable):\n",
    "    >>>     print i, j\n",
    "    0 1\n",
    "    0 2\n",
    "    1 2\n",
    "    \"\"\"\n",
    "    length = len(iterable)\n",
    "    for i in range(len(iterable)):\n",
    "        for j in range(i+1, len(iterable)):\n",
    "            yield (i, j)\n",
    "            \n",
    "def remove_overlapping_records(exonerate_records_from_single_contig, contig):\n",
    "    \n",
    "    records_to_remove = set()\n",
    "\n",
    "    for i,j in iter_half_matrix_indices(exonerate_records_from_single_contig):\n",
    "        ovrlap, which =\\\n",
    "        records_overlapping(exonerate_records_from_single_contig[j],\n",
    "                            exonerate_records_from_single_contig[i])\n",
    "\n",
    "        if ovrlap:\n",
    "            if which == 1:\n",
    "                records_to_remove.add(j)\n",
    "            else:\n",
    "                records_to_remove.add(i)\n",
    "\n",
    "    if len(records_to_remove) > 0:\n",
    "        try:\n",
    "             assert(max(records_to_remove)+1 <= len(exonerate_records_from_single_contig))\n",
    "        except:\n",
    "            raise AssertionError(contig)\n",
    "\n",
    "        records_to_remove = sorted(records_to_remove, reverse=True)\n",
    "\n",
    "        try:\n",
    "            assert(records_to_remove[0] < len(exonerate_records_from_single_contig))\n",
    "        except:\n",
    "            raise AssertionError(contig)\n",
    "\n",
    "        try:\n",
    "            assert(records_to_remove[0] == max(records_to_remove))\n",
    "        except:\n",
    "            raise AssertionError(contig)\n",
    "\n",
    "        for i in records_to_remove:\n",
    "            exonerate_records_from_single_contig.pop(i)\n",
    "    return exonerate_records_from_single_contig\n",
    "                \n",
    "def parse_several_exonerate_output_files(outfiles_dir,\n",
    "                                         cds_fpath,\n",
    "                                         gene_fpath,\n",
    "                                         gff_fpath,\n",
    "                                         q):\n",
    "    from glob import glob\n",
    "    import sys\n",
    "    from Bio import SeqIO\n",
    "    from StringIO import StringIO\n",
    "\n",
    "    outfiles = glob(\"%s/exoout*\"%outfiles_dir)\n",
    "    \n",
    "    # q is proteins, we write CDSs\n",
    "    lengths = {s.id: len(s.seq)*3 for s in SeqIO.parse(q,'fasta')}\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for f in outfiles:\n",
    "        for record in iter_records(f):\n",
    "            #print record\n",
    "            r = parse_a_single_record(record)\n",
    "            records.append(r)\n",
    "\n",
    "\n",
    "    records = sorted(records,\n",
    "\n",
    "                     key=lambda r: (r['Target'],\n",
    "                                    r['Target_end'] - r['Target_start']),\n",
    "\n",
    "                     reverse = True)\n",
    "\n",
    "\n",
    "    cds = open(cds_fpath,'wt')   \n",
    "    gene = open(gene_fpath,'wt')\n",
    "    gff = open(gff_fpath,'wt')\n",
    "\n",
    "\n",
    "    contig = records[0]['Target']\n",
    "    seen_contigs = []\n",
    "\n",
    "    contig_records = []\n",
    "    x = 0\n",
    "\n",
    "    for r in records:\n",
    "        if r['Target'] == contig:\n",
    "            contig_records.append(r)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            # Remove matches that are less than 0.95 the query\n",
    "            \n",
    "            records_to_remove = set()\n",
    "            contig_records = filter_out_short_exonerate_matches(contig_records, lengths)\n",
    "            \n",
    "            # Remove one of two overlapping matches based on score\n",
    "            \n",
    "            contig_records = remove_overlapping_records(contig_records, contig)\n",
    "\n",
    "            for cr in contig_records:\n",
    "                cds.write(cr['CDS'].replace('>','>%i.'%x))\n",
    "                gene.write(cr['Gene'].replace('>','>%i.'%x))\n",
    "                gff.write(\"#GlobalCounter%i\\n\"%x+cr['GFF'])\n",
    "                x += 1\n",
    "\n",
    "            seen_contigs.append(contig) \n",
    "\n",
    "            if len(seen_contigs) % 1000 == 0:\n",
    "                print len(seen_contigs)\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            contig = r['Target']\n",
    "            contig_records = [r]\n",
    "\n",
    "    # sort records for the last contig\n",
    "    \n",
    "    # remove short matches\n",
    "    records_to_remove = set()\n",
    "    contig_records = filter_out_short_exonerate_matches(contig_records, lengths)\n",
    "\n",
    "    # Remove one of two overlapping matches based on score\n",
    "\n",
    "    contig_records = remove_overlapping_records(contig_records, contig)\n",
    "\n",
    "    for cr in contig_records:\n",
    "        cds.write(cr['CDS'].replace('>','>%i.'%x))\n",
    "        gene.write(cr['Gene'].replace('>','>%i.'%x))\n",
    "        gff.write(\"#GlobalCounter%i\\n\"%x+cr['GFF'])\n",
    "        x += 1\n",
    "\n",
    "    seen_contigs.append(contig) \n",
    "\n",
    "    if len(seen_contigs) % 1000 == 0:\n",
    "        print len(seen_contigs)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    cds.close()\n",
    "    gene.close()\n",
    "    gff.close() \n",
    "\n",
    "    assert len(seen_contigs) == len(set(seen_contigs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 Run exonerate for SJF1\n",
    "A preliminary phylogenetic analysis using snps shows that the Nuclear genome of SJF1 is only slightly diverged from that of W1. I am running exonerate to annotate the Mkon assembly with the W1 proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run exonerate\n",
    "q = 'protein_ref_file/W1_ref.aa.fasta'\n",
    "t = assembly_files['MfloSJF1_ref']\n",
    "outdir = \"./SJF1_exonerate_outputs/\"\n",
    "threads = 16\n",
    "\n",
    "# This will be skipped if the output already exists\n",
    "run_exonerate(q, t, outdir, threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.3 parse SJF1 exonerate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from warnings import warn\n",
    "\n",
    "cds_exists = os.path.exists('./cds_ref_files/MfloSJF1_ref.cds.fasta')\n",
    "gene_exists = os.path.exists('./gene_ref_files/MfloSJF1_ref.nt.fasta')\n",
    "gff_exists = os.path.exists('./gff_ref_files/MfloSJF1_ref.gff') \n",
    "input_exists = len(list(glob('./SJF1_exonerate_outputs/exoout*'))) > 0\n",
    "q = 'protein_ref_file/W1_ref.aa.fasta'\n",
    "\n",
    "if any([cds_exists,\n",
    "        gene_exists, \n",
    "        gff_exists, \n",
    "        not input_exists]):\n",
    "    \n",
    "    warn('Some input is missing or output exists')\n",
    "\n",
    "else:\n",
    "    parse_several_exonerate_output_files('./SJF1_exonerate_outputs/',\n",
    "                                        './cds_ref_files/MfloSJF1_ref.cds.fasta',\n",
    "                                        './gene_ref_files/MfloSJF1_ref.nt.fasta',\n",
    "                                        './gff_ref_files/MfloSJF1_ref.gff',\n",
    "                                        q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.6 Make protein file for MfloSJF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from warnings import warn\n",
    "from Bio import SeqIO\n",
    "\n",
    "for smpl in ['MfloSJF1']:\n",
    "    \n",
    "    cds_file = './cds_ref_files/%s_ref.cds.fasta'%smpl\n",
    "    prot_file = './protein_ref_file/%s_ref.aa.fasta'%smpl\n",
    "    \n",
    "    cds_exists = os.path.exists(cds_file)\n",
    "    prot_exists = os.path.exists(prot_file)\n",
    "    \n",
    "    if not cds_exists:\n",
    "        warn('input missing')\n",
    "        continue\n",
    "        \n",
    "    if prot_exists:\n",
    "        warn('output_exists')\n",
    "        continue\n",
    "    \n",
    "    cdss = SeqIO.parse(cds_file,'fasta')\n",
    "    prot_hndl = open(prot_file,'wt')\n",
    "    \n",
    "    for cds in cdss:\n",
    "    \n",
    "        prot_seq = str(cds.seq.translate())\n",
    "\n",
    "        if prot_seq.endswith('*'):\n",
    "            prot_seq = prot_seq[:-1]\n",
    "\n",
    "        prot_hndl.write('>%s\\n%s\\n'%(cds.id, prot_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Quality checks\n",
    "### 1.6.1 Check the number of protein sequences in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protein_ref_file/MincW1_ref.aa.fasta 24714\n",
      "protein_ref_file/MentL30_ref.aa.fasta 31051\n",
      "protein_ref_file/MfloJB5_ref.aa.fasta 15114\n",
      "protein_ref_file/Mkon_ref.aa.fasta 38995\n",
      "protein_ref_file/MjavVW4_ref.aa.fasta 25697\n",
      "protein_ref_file/MfloSJF1_ref.aa.fasta 14399\n",
      "protein_ref_file/MareHarA_ref.aa.fasta 30286\n",
      "cds_ref_files/MareHarA_ref.cds.fasta 30299\n",
      "cds_ref_files/MfloSJF1_ref.cds.fasta 14408\n",
      "cds_ref_files/MfloJB5_ref.cds.fasta 15119\n",
      "cds_ref_files/MjavVW4_ref.cds.fasta 25697\n",
      "cds_ref_files/MentL30_ref.cds.fasta 31051\n",
      "cds_ref_files/Mkon_ref.cds.fasta 38995\n",
      "cds_ref_files/MincW1_ref.cds.fasta 24714\n",
      "gene_ref_files/MjavVW4_ref.nt.fasta 25697\n",
      "gene_ref_files/MfloSJF1_ref.nt.fasta 14414\n",
      "gene_ref_files/MfloJB5_ref.nt.fasta 14747\n",
      "gene_ref_files/Mkon_ref.nt.fasta 38995\n",
      "gene_ref_files/MareHarA_ref.nt.fasta 30304\n",
      "gene_ref_files/MentL30_ref.nt.fasta 31051\n",
      "gene_ref_files/MincW1_ref.nt.fasta 24714\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import sys\n",
    "for f in glob('protein_ref_file/*'):\n",
    "    print f, len(list(SeqIO.parse(f,'fasta')))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "for f in glob('cds_ref_files/*'):\n",
    "    print f, len(list(SeqIO.parse(f,'fasta')))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "for f in glob('gene_ref_files/*'):\n",
    "    print f, len(list(SeqIO.parse(f,'fasta')))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2 Remove redundant sequences\n",
    "Identify clusters of near identical coding sequences and retain only the longest in the cluster, using vsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import misc\n",
    "\n",
    "misc.makedir('cds_ref_centroids/')\n",
    "vsearch_template = \"vsearch --cluster_fast cds_ref_files/{0}.cds.fasta --id 0.98 \"\n",
    "vsearch_template += \"--centroids cds_ref_centroids/{0}.cds.fasta --threads 14\"\n",
    "\n",
    "for ref in assembly_files:\n",
    "    if os.path.exists('cds_ref_centroids/%s.cds.fasta'%ref):\n",
    "        continue\n",
    "    cline = vsearch_template.format(ref)\n",
    "    out, err = misc.execute_cline(cline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.3 confirm CDSs and translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from misc import *\n",
    "from Bio import SeqIO\n",
    "import os, sys\n",
    "from warnings import warn\n",
    "\n",
    "for fpath in ['protein_ref_reviewed',\n",
    "              'cds_ref_reviewed',\n",
    "              'gene_ref_reviewed',\n",
    "              'stopped_protein_ref_reviewed',\n",
    "              'stopped_cds_ref_reviewed',\n",
    "              'stopped_gene_ref_reviewed']:\n",
    "    makedir(fpath)\n",
    "\n",
    "rejected_genes = []    \n",
    "\n",
    "for ref in assembly_files:\n",
    "    \n",
    "    # input file names\n",
    "    prot_in = 'protein_ref_file/'+ref+'.aa.fasta'\n",
    "    cds_in =  'cds_ref_centroids/'+ref+'.cds.fasta'\n",
    "    gene_in = 'gene_ref_files/'+ref+'.nt.fasta'\n",
    "    \n",
    "    # output file names\n",
    "    prot_out = 'protein_ref_reviewed/'+ref+'.aa.fasta'\n",
    "    cds_out =  'cds_ref_reviewed/'+ref+'.cds.fasta'\n",
    "    gene_out = 'gene_ref_reviewed/'+ref+'.nt.fasta'\n",
    "    stopped_prot_out = 'stopped_protein_ref_reviewed/'+ref+'.aa.fasta'\n",
    "    stopped_cds_out =  'stopped_cds_ref_reviewed/'+ref+'.cds.fasta'\n",
    "    stopped_gene_out = 'stopped_gene_ref_reviewed/'+ref+'.nt.fasta'\n",
    "    \n",
    "    # files exist?\n",
    "    prot_in_exists = os.path.exists(prot_in)\n",
    "    cds_in_exists = os.path.exists(cds_in)\n",
    "    gene_in_exists = os.path.exists(gene_in)\n",
    "    \n",
    "    prot_out_exists = os.path.exists(prot_out)\n",
    "    cds_out_exists = os.path.exists(cds_out)\n",
    "    gene_out_exists = os.path.exists(gene_out)\n",
    "    \n",
    "    input_files_exist = [prot_in_exists, cds_in_exists, gene_in_exists]\n",
    "    output_files_exist = [prot_out_exists, cds_out_exists, gene_out_exists]\n",
    "    \n",
    "    # Dont try to work on samples with missing inputs\n",
    "    if not all(input_files_exist):\n",
    "        warn('some or all input files are missing for ref %s. Skipping'%ref)\n",
    "        continue\n",
    "    \n",
    "    # Don't work on samples with output files\n",
    "    if all(output_files_exist):\n",
    "        warn('skipping ref %s. all outputs exist'%ref)\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    if any(output_files_exist):\n",
    "        warn('some output files exist for ref %s. Skipping'%ref)\n",
    "        continue\n",
    "        \n",
    "    # Only work on samples with all the inputs and none of the outputs\n",
    "    if (all(input_files_exist) and not any(output_files_exist)):\n",
    "        \n",
    "        # Get the unfiltered genes, cdss and proteins as SeqRecords\n",
    "        genes = SeqIO.parse('gene_ref_files/%s.nt.fasta'%ref,'fasta')\n",
    "        cdss = SeqIO.to_dict(SeqIO.parse('cds_ref_files/%s.cds.fasta'%ref,'fasta'))\n",
    "        proteins = SeqIO.to_dict(SeqIO.parse('protein_ref_file/%s.aa.fasta'%ref,'fasta'))\n",
    "        \n",
    "        # File handles for filtered reocrds\n",
    "        prot_hndl =   open(prot_out,'wt') \n",
    "        cds_hndl =    open(cds_out ,'wt')\n",
    "        gene_hndl =   open(gene_out,'wt')\n",
    "        stopped_gene_hndl =   open(stopped_gene_out,'wt') \n",
    "        stopped_cds_hndl =   open(stopped_cds_out,'wt') \n",
    "        stopped_prot_hndl =   open(stopped_prot_out,'wt') \n",
    " \n",
    "        for gene in genes:\n",
    "            \n",
    "            # filter out gene if there's no CDS\n",
    "            if not gene.id in cdss:\n",
    "                rejected_genes.append([ref, gene.id,'no cds'])\n",
    "                continue\n",
    "                \n",
    "            # filter out gene if CDS shorter than 90\n",
    "            if not len(cdss[gene.id]) >= 90:\n",
    "                continue\n",
    "            \n",
    "            # filter out gene if there's no protein\n",
    "            if not gene.id in proteins:\n",
    "                rejected_genes.append([ref, gene.id,'no protein'])\n",
    "                continue\n",
    "            \n",
    "            # filter out gene if the CDS seq not identical\n",
    "            # to the gene, except for gaps\n",
    "            with open('temp.fas','wt') as tmp:\n",
    "                tmp.write(gene.format('fasta')+\n",
    "                          cdss[gene.id].format('fasta'))\n",
    "                \n",
    "            align = mafft_align_contigs('temp.fas')\n",
    "            os.remove('temp.fas')\n",
    "            cumulative_entropy = get_aln_cumulative_entropy(align)\n",
    "            if cumulative_entropy > 0:\n",
    "                rejected_genes.append([ref, gene.id,'cds does not match gene'])\n",
    "                continue\n",
    "            \n",
    "            # filter out gene if cds translation and the protein\n",
    "            # sequence from the file are not the same\n",
    "            translated_cds = str(cdss[gene.id].seq.translate())\n",
    "            if translated_cds.endswith('*'):\n",
    "                translated_cds = translated_cds[:-1]\n",
    "            \n",
    "            if not translated_cds == str(proteins[gene.id].seq):\n",
    "                rejected_genes.append([ref, gene.id,'protein does not match cds'])\n",
    "                continue\n",
    "                \n",
    "            # filter out gene if the cds has a stop codon in the middle\n",
    "            # Because I am using proteins for OG clustering and for codon\n",
    "            # alignment in the phylogenetic analysis, I cannot have genes with\n",
    "            # point deleteion/ frameshift. This is a shame because I might be \n",
    "            # lossing a lot of data, but then a direct sequence alignment of CDSs\n",
    "            # is to inaccurate to trust without manual review.\n",
    "            stopped = False\n",
    "            if '*' in str(proteins[gene.id].seq):\n",
    "                stopped = True\n",
    "            \n",
    "            \n",
    "            # if all good, write to files\n",
    "            if stopped:\n",
    "                stopped_gene_hndl.write(gene.format('fasta'))\n",
    "                stopped_cds_hndl.write(cdss[gene.id].format('fasta'))\n",
    "                stopped_prot_hndl.write(proteins[gene.id].format('fasta'))\n",
    "            else:\n",
    "                gene_hndl.write(gene.format('fasta'))\n",
    "                cds_hndl.write(cdss[gene.id].format('fasta'))\n",
    "                prot_hndl.write(proteins[gene.id].format('fasta'))\n",
    "    \n",
    "        prot_hndl.close()\n",
    "        cds_hndl.close()\n",
    "        gene_hndl.close()\n",
    "    \n",
    "        stopped_prot_hndl.close()\n",
    "        stopped_cds_hndl.close()\n",
    "        stopped_gene_hndl.close()\n",
    "        \n",
    "        \n",
    "picdump_compress(rejected_genes,'rejected_genes.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.4 Check the number of protein sequences in the reviewed output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/anaconda2/lib/python2.7/site-packages/pandas/computation/__init__.py:19: UserWarning: The installed version of numexpr 2.4.4 is not supported in pandas and will be not be used\n",
      "\n",
      "  UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>MjavVW4_ref</th>\n",
       "      <th>MincW1_ref</th>\n",
       "      <th>MfloJB5_ref</th>\n",
       "      <th>MentL30_ref</th>\n",
       "      <th>MfloSJF1_ref</th>\n",
       "      <th>MareHarA_ref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gene_ref_files</td>\n",
       "      <td>25697</td>\n",
       "      <td>24714</td>\n",
       "      <td>14747</td>\n",
       "      <td>31051</td>\n",
       "      <td>14414</td>\n",
       "      <td>30304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cds_ref_files</td>\n",
       "      <td>25697</td>\n",
       "      <td>24714</td>\n",
       "      <td>15119</td>\n",
       "      <td>31051</td>\n",
       "      <td>14414</td>\n",
       "      <td>30299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>protein_ref_file</td>\n",
       "      <td>25697</td>\n",
       "      <td>24714</td>\n",
       "      <td>15114</td>\n",
       "      <td>31051</td>\n",
       "      <td>14414</td>\n",
       "      <td>30286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cds_ref_centroids</td>\n",
       "      <td>23870</td>\n",
       "      <td>22871</td>\n",
       "      <td>15055</td>\n",
       "      <td>29781</td>\n",
       "      <td>13871</td>\n",
       "      <td>28219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gene_ref_reviewed</td>\n",
       "      <td>19690</td>\n",
       "      <td>18078</td>\n",
       "      <td>14500</td>\n",
       "      <td>22993</td>\n",
       "      <td>10634</td>\n",
       "      <td>21655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cds_ref_reviewed</td>\n",
       "      <td>19690</td>\n",
       "      <td>18078</td>\n",
       "      <td>14500</td>\n",
       "      <td>22993</td>\n",
       "      <td>10634</td>\n",
       "      <td>21655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>protein_ref_reviewed</td>\n",
       "      <td>19690</td>\n",
       "      <td>18078</td>\n",
       "      <td>14500</td>\n",
       "      <td>22993</td>\n",
       "      <td>10634</td>\n",
       "      <td>21655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stopped_gene_ref_reviewed</td>\n",
       "      <td>4221</td>\n",
       "      <td>4333</td>\n",
       "      <td>0</td>\n",
       "      <td>5039</td>\n",
       "      <td>2590</td>\n",
       "      <td>6401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stopped_cds_ref_reviewed</td>\n",
       "      <td>4221</td>\n",
       "      <td>4333</td>\n",
       "      <td>0</td>\n",
       "      <td>5039</td>\n",
       "      <td>2590</td>\n",
       "      <td>6401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stopped_protein_ref_reviewed</td>\n",
       "      <td>4221</td>\n",
       "      <td>4333</td>\n",
       "      <td>0</td>\n",
       "      <td>5039</td>\n",
       "      <td>2590</td>\n",
       "      <td>6401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           data  MjavVW4_ref  MincW1_ref  MfloJB5_ref  \\\n",
       "0                gene_ref_files        25697       24714        14747   \n",
       "1                 cds_ref_files        25697       24714        15119   \n",
       "2              protein_ref_file        25697       24714        15114   \n",
       "3             cds_ref_centroids        23870       22871        15055   \n",
       "4             gene_ref_reviewed        19690       18078        14500   \n",
       "5              cds_ref_reviewed        19690       18078        14500   \n",
       "6          protein_ref_reviewed        19690       18078        14500   \n",
       "7     stopped_gene_ref_reviewed         4221        4333            0   \n",
       "8      stopped_cds_ref_reviewed         4221        4333            0   \n",
       "9  stopped_protein_ref_reviewed         4221        4333            0   \n",
       "\n",
       "   MentL30_ref  MfloSJF1_ref  MareHarA_ref  \n",
       "0        31051         14414         30304  \n",
       "1        31051         14414         30299  \n",
       "2        31051         14414         30286  \n",
       "3        29781         13871         28219  \n",
       "4        22993         10634         21655  \n",
       "5        22993         10634         21655  \n",
       "6        22993         10634         21655  \n",
       "7         5039          2590          6401  \n",
       "8         5039          2590          6401  \n",
       "9         5039          2590          6401  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "from glob import glob\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "refs = [ref for ref in assembly_files]\n",
    "\n",
    "summary = [['data']+refs]\n",
    "\n",
    "file_types = [\n",
    "    ['gene_ref_files','nt'],\n",
    "    ['cds_ref_files','cds'],\n",
    "    ['protein_ref_file','aa'],\n",
    "    ['cds_ref_centroids','cds'],\n",
    "    ['gene_ref_reviewed','nt'],\n",
    "    ['cds_ref_reviewed','cds'],\n",
    "    ['protein_ref_reviewed','aa'],\n",
    "    ['stopped_gene_ref_reviewed','nt'],\n",
    "    ['stopped_cds_ref_reviewed','cds'],\n",
    "    ['stopped_protein_ref_reviewed','aa']\n",
    "]\n",
    "\n",
    "\n",
    "for ftype in file_types:\n",
    "    counts = [ftype[0]]\n",
    "    for ref in refs:\n",
    "        f = '%s/%s.%s.fasta'%(ftype[0],ref,ftype[1])\n",
    "        counts.append(len(list(SeqIO.parse(f,'fasta'))))\n",
    "    summary.append(counts)\n",
    "    \n",
    "header = summary.pop(0)\n",
    "df = pd.DataFrame(summary, columns=header)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "misc.py:291: UserWarning: keeping existing fpath all_gene_ref_reviewed\n",
      "  warnings.warn('keeping existing fpath %s'%name)\n",
      "misc.py:291: UserWarning: keeping existing fpath all_protein_ref_reviewed\n",
      "  warnings.warn('keeping existing fpath %s'%name)\n",
      "misc.py:291: UserWarning: keeping existing fpath all_cds_ref_reviewed\n",
      "  warnings.warn('keeping existing fpath %s'%name)\n"
     ]
    }
   ],
   "source": [
    "import misc\n",
    "\n",
    "misc.makedir('all_gene_ref_reviewed')\n",
    "\n",
    "cline = \"cat gene_ref_reviewed/{0}.nt.fasta \"\n",
    "cline += \"stopped_gene_ref_reviewed/{0}.nt.fasta > \"\n",
    "cline += \"all_gene_ref_reviewed/{0}.nt.fasta\"\n",
    "\n",
    "for smpl in assembly_files:\n",
    "    \n",
    "    misc.execute_cline(cline.format(smpl))\n",
    "\n",
    "import misc\n",
    "\n",
    "misc.makedir('all_protein_ref_reviewed')\n",
    "\n",
    "cline = \"cat protein_ref_reviewed/{0}.aa.fasta \"\n",
    "cline += \"stopped_protein_ref_reviewed/{0}.aa.fasta > \"\n",
    "cline += \"all_protein_ref_reviewed/{0}.aa.fasta\"\n",
    "\n",
    "for smpl in assembly_files:\n",
    "    \n",
    "    misc.execute_cline(cline.format(smpl))\n",
    "    \n",
    "import misc\n",
    "\n",
    "misc.makedir('all_cds_ref_reviewed')\n",
    "\n",
    "cline = \"cat cds_ref_reviewed/{0}.cds.fasta \"\n",
    "cline += \"stopped_cds_ref_reviewed/{0}.cds.fasta > \"\n",
    "cline += \"all_cds_ref_reviewed/{0}.cds.fasta\"\n",
    "\n",
    "for smpl in assembly_files:\n",
    "    \n",
    "    misc.execute_cline(cline.format(smpl))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
